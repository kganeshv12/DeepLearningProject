{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendatasets --upgrade --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import opendatasets as od\n",
    "\n",
    "# Assign the Kaggle data set URL into variable\n",
    "dataset = 'https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images'\n",
    "# Using opendatasets let's download the data sets\n",
    "od.download(dataset)\n",
    "# # {\"username\":\"kganeshv\",\"key\":\"224eab20b74a0a68fc7194190f513131\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning CNN for AI-generated Image Detection\n",
    "In this notebook, we will explore the task of classifying images as real or AI-generated using fine-tuning techniques with several well-known CNN architectures. For this classification task, we will use the CIFAKE dataset, which includes both real and AI-generated images.\n",
    "\n",
    "Link for the dataset: [CIFAKE: Real and AI-Generated Synthetic Images](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"cifake-real-and-ai-generated-synthetic-images\" # For Kaggle notebooks. If you run locally, point this line to the CIFAKE directory\n",
    "print(\"Loading dataset from: \" + dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPUs are available for training\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 32\n",
    "img_width = 32\n",
    "batch_size = 500\n",
    "\n",
    "# Load the training data\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir + \"/train\",\n",
    "  seed = 512,\n",
    "  image_size = (img_height, img_width),\n",
    "  batch_size = batch_size)\n",
    "\n",
    "# Load the validation data\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir + \"/test\",\n",
    "  seed = 512,\n",
    "  image_size = (img_height, img_width),\n",
    "  batch_size = batch_size)\n",
    "\n",
    "print(\"Training Classes:\")\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "print(\"Testing Classes:\")\n",
    "class_names = val_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger batch sizes are expected to result in significantly longer training times, especially with a substantial training set of 100,000 images. However, the compact size of the images effectively reduces the impact on training duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting the error rate and metrics rate\n",
    "def plot_metrics(history, metric):\n",
    "    plt.plot(history.history[metric], label = metric)\n",
    "    plt.plot(history.history['val_' + metric], label='val_' + metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Constant values that will be shared by all the models\n",
    "val_true_classes = np.concatenate([y for x, y in val_ds], axis = 0)  # Get true labels\n",
    "class_names = ['FAKE', 'REAL']\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "All the models share the same architecture for a fair comparison: the input layer, followed by the base model with pre-trained weights from the imagenet, then a few dense layers, and then a unit output with a sigmoid activation function.\n",
    "\n",
    "Training is conducted with early stopping criteria monitoring the validation loss, and the best weights will be restored once the training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net_base_model = tf.keras.applications.EfficientNetB4(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (img_height, img_width, 3),\n",
    "    pooling = 'max'\n",
    ")\n",
    "Net_base_model.trainable = True\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape = (img_height, img_width, 3))\n",
    "x = Net_base_model(inputs, training = False)\n",
    "x = BatchNormalization(axis = -1, momentum = 0.99, epsilon = 0.001)(x)\n",
    "x = Dense(256,\n",
    "          kernel_regularizer = regularizers.l2(0.01),\n",
    "          activity_regularizer = regularizers.l1(0.01),\n",
    "          bias_regularizer = regularizers.l1(0.01),\n",
    "          activation = 'relu')(x)\n",
    "x = Dropout(rate = .4, seed = 512)(x)\n",
    "x = Dense(64, activation = 'relu')(x)\n",
    "outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "Net_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "Net_model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adamax(learning_rate = .001),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "Net_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Transfer Learning model\n",
    "print(\"Starting training with Transfer Learning using Net50...\")\n",
    "ResNet_model_history = Net_model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs = 100,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "print(\"Transfer Learning training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "val_loss, val_accuracy, val_precision, val_recall = Net_model.evaluate(val_ds)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Val Loss: {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Val Precision: {val_precision:.4f}\")\n",
    "print(f\"Val Recall: {val_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error rates and metric rates\n",
    "plot_metrics(ResNet_model_history, 'loss')\n",
    "plot_metrics(ResNet_model_history, 'accuracy')\n",
    "plot_metrics(ResNet_model_history, 'precision')\n",
    "plot_metrics(ResNet_model_history, 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels on the validation dataset\n",
    "val_pred_prob = Net_model.predict(val_ds)\n",
    "val_pred = (val_pred_prob > 0.5).astype(int)  # Thresholding probabilities to get binary predictions\n",
    "\n",
    "# Get true labels\n",
    "val_true_classes = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(val_true_classes, val_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, labels_list, figsize=(8, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=labels_list, yticklabels=labels_list)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix if the number of unique labels is less than or equal to 150\n",
    "unique_labels = np.unique(val_true_classes)\n",
    "if len(unique_labels) <= 150:\n",
    "    plot_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(val_true_classes, val_pred, target_names=class_names, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
